{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: tensor([[[[0.2693, 0.2603, 0.2430,  ..., 0.2954, 0.2771, 0.2744],\n",
      "          [0.2708, 0.2800, 0.2754,  ..., 0.2888, 0.2600, 0.3020],\n",
      "          [0.2605, 0.2969, 0.2646,  ..., 0.2795, 0.2428, 0.2563],\n",
      "          ...,\n",
      "          [0.2335, 0.2825, 0.2656,  ..., 0.2800, 0.2681, 0.2859],\n",
      "          [0.2485, 0.2739, 0.2502,  ..., 0.2815, 0.2856, 0.2834],\n",
      "          [0.2771, 0.2703, 0.2969,  ..., 0.2595, 0.2307, 0.3025]],\n",
      "\n",
      "         [[0.2820, 0.2581, 0.2294,  ..., 0.3047, 0.3032, 0.3018],\n",
      "          [0.2612, 0.2898, 0.2864,  ..., 0.2529, 0.2433, 0.3301],\n",
      "          [0.2659, 0.3005, 0.2888,  ..., 0.2681, 0.2314, 0.2698],\n",
      "          ...,\n",
      "          [0.2781, 0.2952, 0.2834,  ..., 0.2705, 0.2649, 0.2815],\n",
      "          [0.2778, 0.2791, 0.2485,  ..., 0.2778, 0.2861, 0.2766],\n",
      "          [0.2830, 0.2534, 0.3054,  ..., 0.2678, 0.2573, 0.2991]],\n",
      "\n",
      "         [[0.2747, 0.2588, 0.2520,  ..., 0.3196, 0.3069, 0.2991],\n",
      "          [0.2747, 0.2791, 0.2483,  ..., 0.2456, 0.2281, 0.3013],\n",
      "          [0.2847, 0.3276, 0.2708,  ..., 0.2720, 0.2502, 0.2891],\n",
      "          ...,\n",
      "          [0.2330, 0.2900, 0.2747,  ..., 0.2732, 0.2496, 0.2959],\n",
      "          [0.2595, 0.2791, 0.2573,  ..., 0.2881, 0.2554, 0.2910],\n",
      "          [0.2593, 0.2466, 0.2703,  ..., 0.3181, 0.2467, 0.3240]],\n",
      "\n",
      "         [[0.2524, 0.2448, 0.2343,  ..., 0.3083, 0.2930, 0.3054],\n",
      "          [0.2480, 0.2781, 0.2859,  ..., 0.2725, 0.2493, 0.2881],\n",
      "          [0.2686, 0.2825, 0.2490,  ..., 0.2654, 0.2588, 0.2759],\n",
      "          ...,\n",
      "          [0.2656, 0.2913, 0.2769,  ..., 0.2910, 0.2179, 0.2988],\n",
      "          [0.2815, 0.2905, 0.2654,  ..., 0.2864, 0.2981, 0.3066],\n",
      "          [0.2759, 0.3042, 0.2808,  ..., 0.2573, 0.2505, 0.3464]],\n",
      "\n",
      "         [[0.2542, 0.2522, 0.2173,  ..., 0.2935, 0.2881, 0.3037],\n",
      "          [0.2600, 0.2700, 0.2891,  ..., 0.2708, 0.2727, 0.3159],\n",
      "          [0.2625, 0.2925, 0.2634,  ..., 0.2676, 0.2573, 0.2625],\n",
      "          ...,\n",
      "          [0.2379, 0.3093, 0.2949,  ..., 0.2871, 0.2292, 0.2871],\n",
      "          [0.2585, 0.3052, 0.2595,  ..., 0.3301, 0.2847, 0.2798],\n",
      "          [0.2542, 0.3044, 0.2991,  ..., 0.2852, 0.2815, 0.3115]]],\n",
      "\n",
      "\n",
      "        [[[0.2815, 0.2620, 0.2729,  ..., 0.2925, 0.2908, 0.2864],\n",
      "          [0.2544, 0.2915, 0.2888,  ..., 0.2433, 0.2939, 0.2898],\n",
      "          [0.3105, 0.2866, 0.2778,  ..., 0.2634, 0.2939, 0.2915],\n",
      "          ...,\n",
      "          [0.2163, 0.3103, 0.3022,  ..., 0.2622, 0.2571, 0.2871],\n",
      "          [0.2637, 0.2852, 0.2566,  ..., 0.2695, 0.2712, 0.2681],\n",
      "          [0.2744, 0.2379, 0.2935,  ..., 0.2520, 0.2549, 0.3035]],\n",
      "\n",
      "         [[0.2683, 0.2448, 0.2395,  ..., 0.2830, 0.2979, 0.2800],\n",
      "          [0.2852, 0.2676, 0.2433,  ..., 0.2325, 0.2581, 0.3142],\n",
      "          [0.3159, 0.2903, 0.2864,  ..., 0.2839, 0.2766, 0.2998],\n",
      "          ...,\n",
      "          [0.2433, 0.2798, 0.2874,  ..., 0.2430, 0.2494, 0.2747],\n",
      "          [0.2883, 0.2671, 0.2656,  ..., 0.2891, 0.2487, 0.2549],\n",
      "          [0.2673, 0.2744, 0.3005,  ..., 0.2959, 0.2556, 0.3442]],\n",
      "\n",
      "         [[0.2546, 0.2654, 0.2573,  ..., 0.2920, 0.2603, 0.3105],\n",
      "          [0.2678, 0.2852, 0.2847,  ..., 0.2500, 0.2255, 0.3125],\n",
      "          [0.3135, 0.2913, 0.2411,  ..., 0.2720, 0.2527, 0.3049],\n",
      "          ...,\n",
      "          [0.2546, 0.3003, 0.2825,  ..., 0.2825, 0.2437, 0.2859],\n",
      "          [0.2622, 0.2993, 0.2705,  ..., 0.3169, 0.2839, 0.2839],\n",
      "          [0.2642, 0.2898, 0.2812,  ..., 0.2700, 0.2766, 0.2891]],\n",
      "\n",
      "         [[0.2754, 0.2642, 0.2588,  ..., 0.2878, 0.2825, 0.2859],\n",
      "          [0.2737, 0.2637, 0.3210,  ..., 0.2886, 0.2177, 0.3398],\n",
      "          [0.3020, 0.3518, 0.2466,  ..., 0.3308, 0.2493, 0.2812],\n",
      "          ...,\n",
      "          [0.2448, 0.2856, 0.2988,  ..., 0.3052, 0.2375, 0.2754],\n",
      "          [0.2605, 0.2803, 0.2401,  ..., 0.3286, 0.3108, 0.2705],\n",
      "          [0.2925, 0.2961, 0.2756,  ..., 0.2869, 0.2327, 0.3186]],\n",
      "\n",
      "         [[0.2744, 0.2756, 0.2747,  ..., 0.2747, 0.2695, 0.2629],\n",
      "          [0.2808, 0.2668, 0.2651,  ..., 0.2668, 0.2708, 0.2776],\n",
      "          [0.2710, 0.2810, 0.2673,  ..., 0.2673, 0.2683, 0.2600],\n",
      "          ...,\n",
      "          [0.2656, 0.2781, 0.2786,  ..., 0.2737, 0.2791, 0.2637],\n",
      "          [0.2646, 0.2751, 0.2622,  ..., 0.2732, 0.2600, 0.2798],\n",
      "          [0.2800, 0.2717, 0.2664,  ..., 0.2642, 0.2720, 0.2644]]],\n",
      "\n",
      "\n",
      "        [[[0.2559, 0.2832, 0.2415,  ..., 0.2959, 0.2646, 0.2637],\n",
      "          [0.2795, 0.2856, 0.2874,  ..., 0.2520, 0.2283, 0.2715],\n",
      "          [0.2642, 0.3188, 0.2433,  ..., 0.3123, 0.2527, 0.2825],\n",
      "          ...,\n",
      "          [0.2482, 0.2842, 0.2695,  ..., 0.2776, 0.2864, 0.2834],\n",
      "          [0.2517, 0.3110, 0.2837,  ..., 0.2822, 0.2932, 0.2915],\n",
      "          [0.2832, 0.2878, 0.2754,  ..., 0.2771, 0.2539, 0.2764]],\n",
      "\n",
      "         [[0.2769, 0.2625, 0.2712,  ..., 0.2759, 0.2935, 0.2469],\n",
      "          [0.2798, 0.2666, 0.2664,  ..., 0.2581, 0.2278, 0.2942],\n",
      "          [0.3154, 0.3154, 0.2844,  ..., 0.2891, 0.2494, 0.3027],\n",
      "          ...,\n",
      "          [0.2399, 0.2932, 0.2905,  ..., 0.2761, 0.2299, 0.2778],\n",
      "          [0.2605, 0.2874, 0.2681,  ..., 0.3091, 0.2534, 0.2612],\n",
      "          [0.2458, 0.2861, 0.2881,  ..., 0.2686, 0.2742, 0.2910]],\n",
      "\n",
      "         [[0.2795, 0.2590, 0.2510,  ..., 0.3032, 0.3401, 0.2510],\n",
      "          [0.2903, 0.2542, 0.2749,  ..., 0.2433, 0.2246, 0.3025],\n",
      "          [0.3110, 0.3235, 0.3018,  ..., 0.3032, 0.2340, 0.2944],\n",
      "          ...,\n",
      "          [0.2522, 0.2849, 0.3108,  ..., 0.3069, 0.2524, 0.2637],\n",
      "          [0.2808, 0.2810, 0.2460,  ..., 0.3062, 0.2512, 0.2859],\n",
      "          [0.2549, 0.2507, 0.2856,  ..., 0.2761, 0.2900, 0.3137]],\n",
      "\n",
      "         [[0.2744, 0.2756, 0.2747,  ..., 0.2747, 0.2695, 0.2629],\n",
      "          [0.2808, 0.2668, 0.2651,  ..., 0.2668, 0.2708, 0.2776],\n",
      "          [0.2710, 0.2810, 0.2673,  ..., 0.2673, 0.2683, 0.2600],\n",
      "          ...,\n",
      "          [0.2656, 0.2781, 0.2786,  ..., 0.2737, 0.2791, 0.2637],\n",
      "          [0.2646, 0.2751, 0.2622,  ..., 0.2732, 0.2600, 0.2798],\n",
      "          [0.2800, 0.2717, 0.2664,  ..., 0.2642, 0.2720, 0.2644]],\n",
      "\n",
      "         [[0.2744, 0.2756, 0.2747,  ..., 0.2747, 0.2695, 0.2629],\n",
      "          [0.2808, 0.2668, 0.2651,  ..., 0.2668, 0.2708, 0.2776],\n",
      "          [0.2710, 0.2810, 0.2673,  ..., 0.2673, 0.2683, 0.2600],\n",
      "          ...,\n",
      "          [0.2656, 0.2781, 0.2786,  ..., 0.2737, 0.2791, 0.2637],\n",
      "          [0.2646, 0.2751, 0.2622,  ..., 0.2732, 0.2600, 0.2798],\n",
      "          [0.2800, 0.2717, 0.2664,  ..., 0.2642, 0.2720, 0.2644]]],\n",
      "\n",
      "\n",
      "        [[[0.2612, 0.2390, 0.2717,  ..., 0.2903, 0.3311, 0.2747],\n",
      "          [0.2686, 0.2634, 0.2732,  ..., 0.2583, 0.2428, 0.2998],\n",
      "          [0.2908, 0.3196, 0.2659,  ..., 0.2649, 0.2847, 0.2832],\n",
      "          ...,\n",
      "          [0.2397, 0.2859, 0.3152,  ..., 0.2812, 0.2571, 0.2401],\n",
      "          [0.2986, 0.2864, 0.2510,  ..., 0.2710, 0.2583, 0.2732],\n",
      "          [0.2622, 0.2766, 0.2498,  ..., 0.2761, 0.2413, 0.3364]],\n",
      "\n",
      "         [[0.2399, 0.2319, 0.2698,  ..., 0.3093, 0.3350, 0.2494],\n",
      "          [0.2727, 0.2480, 0.3037,  ..., 0.2847, 0.2338, 0.2959],\n",
      "          [0.3027, 0.3040, 0.2605,  ..., 0.2690, 0.2581, 0.2405],\n",
      "          ...,\n",
      "          [0.2417, 0.3069, 0.2766,  ..., 0.2988, 0.2686, 0.2462],\n",
      "          [0.2678, 0.2805, 0.2493,  ..., 0.2654, 0.2812, 0.3076],\n",
      "          [0.2700, 0.2817, 0.2483,  ..., 0.2561, 0.2170, 0.3000]],\n",
      "\n",
      "         [[0.2744, 0.2756, 0.2747,  ..., 0.2747, 0.2695, 0.2629],\n",
      "          [0.2808, 0.2668, 0.2651,  ..., 0.2668, 0.2708, 0.2776],\n",
      "          [0.2710, 0.2810, 0.2673,  ..., 0.2673, 0.2683, 0.2600],\n",
      "          ...,\n",
      "          [0.2656, 0.2781, 0.2786,  ..., 0.2737, 0.2791, 0.2637],\n",
      "          [0.2646, 0.2751, 0.2622,  ..., 0.2732, 0.2600, 0.2798],\n",
      "          [0.2800, 0.2717, 0.2664,  ..., 0.2642, 0.2720, 0.2644]],\n",
      "\n",
      "         [[0.2744, 0.2756, 0.2747,  ..., 0.2747, 0.2695, 0.2629],\n",
      "          [0.2808, 0.2668, 0.2651,  ..., 0.2668, 0.2708, 0.2776],\n",
      "          [0.2710, 0.2810, 0.2673,  ..., 0.2673, 0.2683, 0.2600],\n",
      "          ...,\n",
      "          [0.2656, 0.2781, 0.2786,  ..., 0.2737, 0.2791, 0.2637],\n",
      "          [0.2646, 0.2751, 0.2622,  ..., 0.2732, 0.2600, 0.2798],\n",
      "          [0.2800, 0.2717, 0.2664,  ..., 0.2642, 0.2720, 0.2644]],\n",
      "\n",
      "         [[0.2744, 0.2756, 0.2747,  ..., 0.2747, 0.2695, 0.2629],\n",
      "          [0.2808, 0.2668, 0.2651,  ..., 0.2668, 0.2708, 0.2776],\n",
      "          [0.2710, 0.2810, 0.2673,  ..., 0.2673, 0.2683, 0.2600],\n",
      "          ...,\n",
      "          [0.2656, 0.2781, 0.2786,  ..., 0.2737, 0.2791, 0.2637],\n",
      "          [0.2646, 0.2751, 0.2622,  ..., 0.2732, 0.2600, 0.2798],\n",
      "          [0.2800, 0.2717, 0.2664,  ..., 0.2642, 0.2720, 0.2644]]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class TemporalRCNN(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, map_h, map_w):\n",
    "        super(TemporalRCNN, self).__init__()\n",
    "\n",
    "        # Spatial feature extraction layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((map_h, map_w))  # Reduce to [8x8]\n",
    "\n",
    "        # LSTM for temporal dependencies\n",
    "        self.lstm = nn.LSTM(input_size=64 * map_h * map_w, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        # Project LSTM output to spatial probability maps\n",
    "        self.fc = nn.Linear(hidden_dim, map_h * map_w)  # Project to spatial map\n",
    "        self.conv3 = nn.Conv2d(1, 1, kernel_size=1)  # Optional: Smooth output\n",
    "        \n",
    "        self.map_h = map_h\n",
    "        self.map_w = map_w\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, channels, height, width = x.size()  # [B, T, C, H, W]\n",
    "\n",
    "        # Process each frame independently with convolutional layers\n",
    "        x = x.view(batch_size * seq_len, channels, height, width)  # [B*T, C, H, W]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        # Apply adaptive pooling to reduce spatial size\n",
    "        x = self.gap(x)  # [B*T, 64, 8, 8]\n",
    "        x = x.view(batch_size, seq_len, -1)  # [B, T, 64*8*8]\n",
    "\n",
    "        # Pack the sequences to handle variable lengths\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # LSTM processes temporal information\n",
    "        lstm_out, _ = self.lstm(x_packed)  # [B, T, hidden_dim]\n",
    "\n",
    "        # Unpack the sequences\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)  # [B, T, hidden_dim]\n",
    "\n",
    "        # Project LSTM output to spatial dimensions using the fully connected layer\n",
    "        lstm_out = self.fc(lstm_out)  # [B, T, 8*8]\n",
    "        lstm_out = lstm_out.view(batch_size * seq_len, 1, self.map_h, self.map_w)  # [B*T, 1, 8, 8]\n",
    "\n",
    "        # Optionally apply a 1x1 convolution to smooth the output\n",
    "        prob_map = torch.sigmoid(self.conv3(lstm_out))  # [B*T, 1, 8, 8]\n",
    "\n",
    "        # Reshape back to sequence format\n",
    "        prob_map = prob_map.view(batch_size, seq_len, self.map_h, self.map_w)  # [B, T, 8, 8]\n",
    "\n",
    "        return prob_map\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input: batch of 4 sequences, each with 5 frames, 3 channels, and 32x32 resolution\n",
    "    input_data = torch.randn(4, 5, 3, 32, 32)  # [B, T, C, H, W]\n",
    "\n",
    "    # Sequence lengths (for variable-length sequences)\n",
    "    lengths = torch.tensor([5, 4, 3, 2])  # Different sequence lengths for each batch sample\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TemporalRCNN(input_channels=3, hidden_dim=128, map_h=16, map_w=16)\n",
    "\n",
    "    # Move model and data to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    input_data = input_data.to(device).half()  # Half-precision\n",
    "\n",
    "    # Forward pass with mixed precision\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output = model(input_data, lengths)\n",
    "\n",
    "    print(\"Output shape:\", output)  # Should be [batch_size, seq_len, 8, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8076171875\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "# Example target: [batch_size, seq_len, 8, 8]\n",
    "target = torch.randint(0, 2, (4, 5, 16, 16)).float().to(device).half()  # Binary labels\n",
    "\n",
    "# Forward pass to get predictions\n",
    "with torch.cuda.amp.autocast():\n",
    "    output = model(input_data, lengths)  # [B, T, 8, 8]\n",
    "\n",
    "# Compute BCE loss\n",
    "loss = criterion(output, target) # Half-precision\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
